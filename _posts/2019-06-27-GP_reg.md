---
title: "Gaussian Process Regression"
date: 2019-06-27
mathjax: true
---

<script src="//yihui.name/js/math-code.js"></script>
<!-- Just one possible MathJax CDN below. You may use others. -->
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- Numbering equations. -->

Gaussian Process Regression is a really flexible way to learn about the parameters of some unknown, unobservable function. When I first came across these I was really confused. I think this mostly to do with the fact that being an Economics student I only ever really learnt about regression from the perspective of frequentist linear regression... also my grasp of linear algebra could be better. Hopefully this guide will be useful to some of you that come from a similar perspective, and I will do my best to illustrate how GP's are actually similar in flavour to other methods of estimating unkown functions such as OLS (which we all know and love). 

Firstly, Gaussian Process Regression is a **Bayesian** approach. This means that we treat parameters in our model as random variables rather than constants. For example, for a statistical model of the following form:

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \cdots \beta_k x_k + u $$

\\(\beta \\) is now a random variable which has some distribution, for example it may be the case that \\(\beta \sim N(0, \Sigma_k)\\). This is **not** the case in standard frequentist approaches you may have come across in statistics or econometrics classes. 

For a second let's discuss a 'Gaussian Process'. A stochastic process can be defined as a collection of random variables, for example a time series is a stochastic processes that is indexed over time. Now, a Gaussian Process is a collection of random variables that are jointly normally distributed. 

A Gaussian Process is defined completely by it's mean and covariance functions:

\begin{equation}
f(\mathbf{x}) \sim GP\big(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}')  \big)
\label{eq:GP}
\end{equation}



We can think of the covariance function (or 'kernel') \\(k(\mathbf{x}, \mathbf{x}') \\) as defining the covariance or similarity between function outputs. That is, the covariance function for  \\(k(x_1, x_2) \\) defines how similar the points \\(x_1\\) and \\(x_2\\) in terms of the function outputs. In layman terms the covariance function defines the 'shape' of the GP. 

A popular choice for the covariance function is the *squared exponential* where:

$$ k(x_p, x_q) = \exp (- \frac{1}{2} | x_p - x_q|^2 ) $$

It is possible to show (through some fairly heavy linear algebra) that a Gaussian Process with this choice of kernel corresponds with Bayesian linear regression with an **infinite** amount of basis functions. That is, a model which has an infinite ammount of polynomials of \\(x \\):

$$ y = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots $$

The proof can be found in section 4.3.1 of [Rasmussen](http://www.gaussianprocess.org/gpml/chapters/RW.pdf) 

This essentially means that using a Gaussian Process gives us quite a lot of flexibility with respect to the 'shapes' of functions that are admissible in learning about the underlying function. We are not just restricted to a linear model. 

The specification of a GP in equation \eqref{eq:GP} implies a distribution over functions. We can 'draw functions' from this distribution by specifying a number of data points \mathbf{x} (for example, equidistant points on the [-5, 5] interval, calculating a kernel matrix and then drawing sample from a multivariate normal distribution. 

{% highlight python %}
#generating samples from a GP prior

import numpy as np

#equidistant points
x = np.arange(-5,5,0.1)

#SE function for 1d space
def Squared_Exponential(variance, lengthscales, x1, x2):
    '''Calculates squared exponential kernel function for a given variance, lengthscale(s) and distance  '''
    k = variance*np.exp((-1/(2*lengthscales))*(x1 - x2)**2)    
    return(k)

#constructing covariance matrix by applying SE function element wise

n = len(x)
K = np.zeros((n,n))

for i in range(0,n):
    for j in range(0,n):
        #euclidean distance
        K[i,j] = Squared_Exponential(variance = 1, lengthscales = 1, x1 = x[i], x2 = x[j])

f = np.random.multivariate_normal(mean = np.zeros(n), cov = K)
import matplotlib.pyplot as plt

scatter = plt.scatter(x, f, 15, 'red', alpha = 0.5)
line = plt.plot(x,f)
plt.title('A function drawn from a GP prior')
plt.xlabel('Input: x')
plt.ylabel('Output: f(x)')
plt.legend([scatter, line[0]], ['Actual Sampled Points', 'Hypothetical Function'], loc = 'lower left')
#plt.savefig('GPprior.png')

{% endhighlight %}


<div style="text-align: center"><img src="/assets/GPprior.png" width="400" /></div>

## Sources:

Rasumussen: [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/chapters/RW.pdf)
Catherine Bailey: [Gaussian Processes for Dummies](https://katbailey.github.io/post/gaussian-processes-for-dummies/)

