---
title: "Gaussian Process Regression: Introduction"
date: 2019-06-27
mathjax: true
---

<script src="//yihui.name/js/math-code.js"></script>
<!-- Just one possible MathJax CDN below. You may use others. -->
<script async
  src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- Numbering equations. -->

Gaussian Process Regression is a really flexible way to learn about the parameters of some unknown, unobservable function. When I first came across these I was really confused. I think this mostly to do with the fact that being an Economics student I only ever really learnt about regression from the perspective of frequentist linear regression... also my grasp of linear algebra could be better. Hopefully this guide will be useful to some of you that come from a similar perspective, and I will do my best to illustrate how GP's are actually similar in flavour to other methods of estimating unkown functions such as OLS (which we all know and love). 

Firstly, Gaussian Process Regression is a **Bayesian** approach. This means that we treat parameters in our model as random variables rather than constants. For example, for a statistical model of the following form:

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \cdots \beta_k x_k + u $$

\\(\beta \\) is now a random variable which has some distribution, for example it may be the case that \\(\beta \sim N(0, \Sigma_k)\\). This is **not** the case in standard frequentist approaches you may have come across in statistics or econometrics classes. 

For a second let's discuss a 'Gaussian Process'. A stochastic process can be defined as a collection of random variables, for example a time series is a stochastic processes that is indexed over time. Now, a Gaussian Process is a collection of random variables that are jointly normally distributed. 

A Gaussian Process is defined completely by it's mean and covariance functions:

\begin{equation}
f(\mathbf{x}) \sim GP\big(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}')  \big)
\label{eq:GP}
\end{equation}



We can think of the covariance function (or 'kernel') \\(k(\mathbf{x}, \mathbf{x}') \\) as defining the covariance or similarity between function outputs. That is, the covariance function for  \\(k(x_1, x_2) \\) defines how similar the points \\(x_1\\) and \\(x_2\\) in terms of the function outputs. In layman terms the covariance function defines the 'shape' of the GP. 

A popular choice for the covariance function is the *squared exponential* where:

$$ k(x_p, x_q) = \exp (- \frac{1}{2} |x_p - x_q|^2 ) $$

It is possible to show (through some fairly heavy linear algebra) that a Gaussian Process with this choice of kernel corresponds with Bayesian linear regression with an **infinite** amount of basis functions. That is, a model which has an infinite ammount of polynomials of \\(x \\):

$$ y = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots $$

The proof can be found in section 4.3.1 of [Rasmussen](http://www.gaussianprocess.org/gpml/chapters/RW.pdf) 

This essentially means that using a Gaussian Process gives us quite a lot of flexibility with respect to the 'shapes' of functions that are admissible in learning about the underlying function. We are not just restricted to a linear model. 

The specification of a GP in equation \eqref{eq:GP} implies a distribution over functions. We can 'draw functions' from this distribution by specifying a number of data points \mathbf{x} (for example, equidistant points on the [-5, 5] interval, calculating a kernel matrix and then drawing sample from a multivariate normal distribution. 

{% highlight python %}
#generating samples from a GP prior

import numpy as np

#equidistant points
x = np.arange(-5,5,0.1)

#SE function for 1d space
def Squared_Exponential(variance, lengthscales, x1, x2):
    '''Calculates squared exponential kernel function for a given variance, lengthscale(s) and distance  '''
    k = variance*np.exp((-1/(2*lengthscales))*(x1 - x2)**2)    
    return(k)

#constructing covariance matrix by applying SE function element wise

n = len(x)
K = np.zeros((n,n))

for i in range(0,n):
    for j in range(0,n):
        #euclidean distance
        K[i,j] = Squared_Exponential(variance = 1, lengthscales = 1, x1 = x[i], x2 = x[j])

f = np.random.multivariate_normal(mean = np.zeros(n), cov = K)
import matplotlib.pyplot as plt

#plotting the graph

scatter = plt.scatter(x, f, 15, 'red', alpha = 0.5)
line = plt.plot(x,f)
plt.title('A function drawn from a GP prior')
plt.xlabel('Input: x')
plt.ylabel('Output: f(x)')
plt.legend([scatter, line[0]], ['Actual Sampled Points', 'Hypothetical Function'], loc = 'lower left')


{% endhighlight %}


<div style="text-align: center"><img src="/assets/GPprior.png" width="400" /></div>

Typically for modelling purposes, we would like to draw functions that `agree' with data we have sampled, hence we can extend our analysis to a **conditional** distribution over functions. An intuitive explanation of what is going on here is we are drawing this functions but 'throwing out' those which do not agree with or 'pass through' points which we have sampled. There are linear algebra results which give us this 'predictive distribution', they are tedious so we will skip them, however the main results are similar in nature to \eqref{eq:GP} they are just slightly more complicated once again the exact results can be found on page 19 of [Rasmussen](http://www.gaussianprocess.org/gpml/chapters/RW.pdf). The textbook gives us a convenient algorithm:

<div style="text-align: center"><img src="/assets/rasalgo.png" width="400" /></div>

{% highlight python %}

import numpy as np

#equidistant test points
x_test = np.arange(-5,5,0.1).reshape(-1,1)

#actual training points (which we will condition on)
x_train = np.array([-2, -1, 0, 1, 2]).reshape(-1,1)

# we are defining a function that is unknown to the GP that will generate actual function values
# these are a function of our inputs
# note that for the moment, these are noise free
y = np.sin(x_train).reshape(-1, 1)


#Procedure from Rasmussen (2009)

R = 3 #number of samples we are drawing
colors = ['red', 'blue', 'lime']

for i in range(0,R):
    
    #kernels 
    K = Squared_Exponential(x_train, x_train, param = 1)
    K_s = Squared_Exponential(x_train, x_test, param = 1)
    K_ss = Squared_Exponential(x_test, x_test, param = 1)

    #cholesky decomposition
    n = len(x_train)
    L = np.linalg.cholesky(K + 0.001*np.eye(n)) 
    
    #Rasmussen procedure
    Ly = np.linalg.solve(L, y)
    alpha = np.linalg.solve(L.T, Ly)
    mu = np.matmul(K_s.T, alpha)
    v = np.linalg.solve(L, K_s)

    variance = K_ss - np.dot(v.T, v)

    L_post = np.linalg.cholesky(variance + 0.001*np.eye(len(variance))) 
    
    #generating errors
    U = np.random.multivariate_normal(mean = np.zeros(len(variance)), cov = np.eye(len(variance)))
    U = U.reshape(-1, 1)

    f_post = mu + np.matmul(L_post, U)
    
    # Compute the standard deviation so we can plot it
    # this comes from the formula for the predictive variance at a point
    # the minus term comes from this formula
    s2 = np.diag(K_ss) - np.sum(v**2, axis=0)
    stdv = np.sqrt(s2)
    
    import matplotlib.pyplot as plt

    #plotting test points
    plt.scatter(x_train, y, marker = 'x', color = 'black', alpha = 1, linewidth = 3)
    #plotting lines
    plt.plot(x_test, f_post, alpha = 0.4, linewidth = 2, color = colors[i])
    #plotting error band
    plt.fill_between(x_test.reshape(100,), mu.reshape(100,) - 2*stdv, mu.reshape(100,) + 2*stdv, color="#dddddd", alpha = 0.2)
    
    plt.title('Drawing functions from a GP Posterior')
    plt.xlabel('Input: x')
    plt.ylabel("Output: $GP \sim N(\mu, k(x,x'))$")

    {% endhighlight %}

<div style="text-align: center"><img src="/assets/GPpost.png" width="400" /></div>

Here, we can see that the functions that we draw 'agree' with the data points we have sampled. Also note that there is less uncertainty surrounding the unknown function near points we have sampled, but more in areas where we have not observed a sample.  This makes sense intuitivley ! 

I think this point is better illustrated without the 'estimated functions' in the visualisation

<div style="text-align: center"><img src="/assets/GPpostnofunc.png" width="400" /></div>

Whilst this is just an introduction, I plan to give a more concrete example of how to model actual phenomena with GPs in the future !


## Sources:

Rasumussen: [Gaussian Processes for Machine Learning](http://www.gaussianprocess.org/gpml/chapters/RW.pdf)
Catherine Bailey: [Gaussian Processes for Dummies](https://katbailey.github.io/post/gaussian-processes-for-dummies/)

